{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#! -*-coding:utf-8 -*-\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from lib.sde import SDE\n",
    "from lib.utils import visualize_diffusion_process_1d, visualize_diffusion_process_2d, visualize_diffusion_process_2d_marginal, visualize_line\n",
    "from lib.utils import weight_init\n",
    "from lib.data import GMM\n",
    "from lib.evaluate import evaluate_1d, evaluate_2d\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## hyper-parameters\n",
    "T = 4.0                                     # the terminal time\n",
    "sample_size = 10000                         # the number of random samples\n",
    "N = 5                                       # control the number of visualizations\n",
    "h_alphas = [0.0, 1.0, 2.0, 3.0, 4.0]        # candidates for h\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GMM 1D"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_type = \"gmm1d\"\n",
    "reverse_N = 40000                           # number of discretization steps\n",
    "\n",
    "## define data\n",
    "\n",
    "mus = np.array([-1.0, 1.0]).reshape(-1, 1)\n",
    "sigms = np.array([0.1, 0.1]).reshape(-1, 1)\n",
    "ps = np.array([0.5, 0.5])\n",
    "\n",
    "true_data = GMM(mus=mus, sigmas=sigms, ps=ps, data_dim=1)\n",
    "\n",
    "x0, x1 = -3.0, 3.0         # the range of x coordinate to visualize and compute divergences\n",
    "\n",
    "data_dim = true_data.data_dim   # dimension of swissroll (=2)\n",
    "\n",
    "## define sde model\n",
    "beta0 = 1.0                     # beta at t = 0\n",
    "beta1 = 1.0                    # beta at t = T\n",
    "\n",
    "model = SDE(T=T, beta0=beta0, beta1=beta1, sde_type=\"vp\", beta_type=\"linear\", data_dim=data_dim, hidden_dim=50)\n",
    "model.apply(weight_init)\n",
    "\n",
    "## create directories for saving results\n",
    "model_dir = f\"sde_{data_type}_T-{T:.2f}\"\n",
    "output_dir = f\"outputs/{model_dir}\"\n",
    "ckpt_dir = f\"ckpts/{model_dir}\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualize the forward process"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## visualize forward process\n",
    "t_unit = T / N\n",
    "t_schedule = np.arange(N + 1) * t_unit\n",
    "\n",
    "# forward process\n",
    "x_0 = true_data.sample(sample_size)\n",
    "x_0 = torch.from_numpy(x_0).view(-1, data_dim)\n",
    "\n",
    "x_t = list()\n",
    "\n",
    "for t in t_schedule:\n",
    "\n",
    "    x_t.append(model.forward_sde(x_0, t, to_numpy=True))\n",
    "\n",
    "x_t = np.stack(x_t, axis=0)\n",
    "visualize_diffusion_process_1d(xs=x_t, titles=[f\"t={t:.2f}\" for t in t_schedule], savename=None)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define the score corrupter"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class CorruptScore(object):\n",
    "\n",
    "    def __init__(self, T, error_type=\"1\") -> None:\n",
    "        self.T = T\n",
    "        \n",
    "        if error_type == \"1\":\n",
    "            self.err_fun = self._error_1\n",
    "        \n",
    "        elif error_type == \"2\":\n",
    "            self.err_fun = self._error_2\n",
    "        \n",
    "        elif error_type == \"3\":\n",
    "            self.err_fun = self._error_3\n",
    "\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "    def _error_1(self, score, t, eps=1e-2):\n",
    "        return (1 + eps) * score\n",
    "\n",
    "    def _error_2(self, score, t, eps=1e-2):\n",
    "        return (1 + eps * (1 + np.sin(2 * np.pi * t / self.T)) / 2) * score\n",
    "\n",
    "    def _error_3(self, score, t, eps=1e-2):\n",
    "        if t > 0.05 * self.T:\n",
    "            return (1 + eps) * score\n",
    "        return score\n",
    "        \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualiza the reverse process"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# build the score corrupter\n",
    "Corrupter = CorruptScore(T=T, error_type=\"1\")\n",
    "eps = -0.20\n",
    "\n",
    "x_T = torch.randn(sample_size, data_dim)\n",
    "\n",
    "# run the reverse process\n",
    "tilde_x_t = list()\n",
    "\n",
    "for h_alpha in h_alphas:\n",
    "        \n",
    "    tic = time.time()\n",
    "    tilde_x_t.append(model.sample(x_t=x_T, T=0.0, N=reverse_N, to_numpy=True, sf_alpha=h_alpha, exact_score_fn=true_data.exact_score_t, corrupter=Corrupter, eps=eps))\n",
    "    toc = time.time()\n",
    "\n",
    "    print(f\"sampling with alpha={h_alpha:.2f} done, cost {toc - tic:.2f}s\")\n",
    "\n",
    "tilde_x_t = np.stack(tilde_x_t, axis=0)\n",
    "\n",
    "# visualization\n",
    "visualize_diffusion_process_1d(xs=tilde_x_t, titles=[f\"h={h:.2f}\" for h in h_alphas], savename=None, density_func=true_data.p_t, )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = list()\n",
    "\n",
    "for h in np.linspace(start=0.0, stop=h_alphas[-1], num=21, endpoint=True)[::-1]:\n",
    "# for sa in [sf_alpha]:\n",
    "\n",
    "    x_0 = true_data.sample(sample_size)\n",
    "    x_0 = torch.from_numpy(x_0).view(-1, data_dim)\n",
    "    \n",
    "    x_0_gen = model.sample(x_t=x_T, T=0.0, N=reverse_N, to_numpy=True, sf_alpha=h, exact_score_fn=true_data.exact_score_t, corrupter=Corrupter, eps=eps)\n",
    "\n",
    "    js, kl, wd = evaluate_1d(true_data=x_0.detach().numpy(), fake_data=x_0_gen, x0=x0, x1=x1)\n",
    "\n",
    "    print(f\"h={h:.2f}: js={js:.4f}, kl={kl:.4f}, wd={wd:.4f}\")\n",
    "\n",
    "    data.append([h, js, kl, wd])\n",
    "\n",
    "data = np.array(data)\n",
    "\n",
    "visualize_line(data=data[:, 1], yscale=\"log\", xaxis=data[:, 0], xl=r\"$\\mathsf{h}$\", yl=\"Error\", title=\"Error measured with JS divergence\", savename=None)\n",
    "visualize_line(data=data[:, 2], yscale=\"log\", xaxis=data[:, 0], xl=r\"$\\mathsf{h}$\", yl=\"Error\", title=\"Error measured with KL divergence\", savename=None)\n",
    "visualize_line(data=data[:, 3], yscale=\"log\", xaxis=data[:, 0], xl=r\"$\\mathsf{h}$\", yl=\"Error\", title=\"Error measured with Wasserstein distance\", savename=None)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GMM 2D"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_type = \"gmm1d\"\n",
    "reverse_N = 80000           # number of discretization steps\n",
    "\n",
    "# define data\n",
    "\n",
    "mus = np.array([[-1.0, -1.0], [-1.0, 1.0], [1.0, 1.0], [1.0, -1.0],]).reshape(-1, 2)\n",
    "sigms = np.array([0.05, 0.05, 0.05, 0.05]).reshape(-1, 1)\n",
    "ps = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "\n",
    "true_data = GMM(mus=mus, sigmas=sigms, ps=ps, data_dim=2)\n",
    "\n",
    "data_dim = true_data.data_dim\n",
    "\n",
    "x0, x1 = -2.5, 2.5          # the range of x coordinate to visualize and compute divergences\n",
    "y0, y1 = -2.5, 2.5          # the range of y coordinate to visualize and compute divergences\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualize the forward process"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## visualize forward process\n",
    "t_unit = T / N\n",
    "t_schedule = np.arange(N + 1) * t_unit\n",
    "\n",
    "# forward process\n",
    "x_0 = true_data.sample(sample_size)\n",
    "x_0 = torch.from_numpy(x_0).view(-1, data_dim)\n",
    "\n",
    "x_t = list()\n",
    "\n",
    "for t in t_schedule:\n",
    "\n",
    "    x_t.append(model.forward_sde(x_0, t, to_numpy=True))\n",
    "\n",
    "x_t = np.stack(x_t, axis=0)\n",
    "visualize_diffusion_process_2d(xs=x_t, titles=[f\"t={t:.2f}\" for t in t_schedule], savename=None)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# build the score corrupter\n",
    "Corrupter = CorruptScore(T=T, error_type=\"1\")\n",
    "eps = -0.20\n",
    "\n",
    "x_T = torch.randn(sample_size, data_dim)\n",
    "\n",
    "# run the reverse process\n",
    "tilde_x_t = list()\n",
    "\n",
    "for h_alpha in h_alphas:\n",
    "        \n",
    "    tic = time.time()\n",
    "    tilde_x_t.append(model.sample(x_t=x_T, T=0.0, N=reverse_N, to_numpy=True, sf_alpha=h_alpha, exact_score_fn=true_data.exact_score_t, corrupter=Corrupter, eps=eps))\n",
    "    toc = time.time()\n",
    "\n",
    "    print(f\"sampling with alpha={h_alpha:.2f} done, cost {toc - tic:.2f}s\")\n",
    "\n",
    "tilde_x_t = np.stack(tilde_x_t, axis=0)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# visualization of the emprical distribution\n",
    "visualize_diffusion_process_2d(xs=tilde_x_t, titles=[f\"h={h:.2f}\" for h in h_alphas], savename=None)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# visualization of the marginal distribution\n",
    "visualize_diffusion_process_2d_marginal(x_t=tilde_x_t, titles=[f\"h={h:.2f}\" for h in h_alphas], density_func=true_data.marginal, title=None, savename=None)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = list()\n",
    "\n",
    "for h in np.linspace(start=0.0, stop=h_alphas[-1], num=21, endpoint=True)[::-1]:\n",
    "# for sa in [sf_alpha]:\n",
    "\n",
    "    x_0 = true_data.sample(sample_size)\n",
    "    x_0 = torch.from_numpy(x_0).view(-1, data_dim)\n",
    "    \n",
    "    x_0_gen = model.sample(x_t=x_T, T=0.0, N=reverse_N, to_numpy=True, sf_alpha=h, exact_score_fn=true_data.exact_score_t, corrupter=Corrupter, eps=eps)\n",
    "\n",
    "    js, kl, wd = evaluate_2d(true_data=x_0.detach().numpy(), fake_data=x_0_gen, x0=x0, x1=x1, y0=y0, y1=y1)\n",
    "\n",
    "    print(f\"h={h:.2f}: js={js:.4f}, kl={kl:.4f}, wd={wd:.4f}\")\n",
    "\n",
    "    data.append([h, js, kl, wd])\n",
    "\n",
    "data = np.array(data)\n",
    "\n",
    "visualize_line(data=data[:, 1], yscale=\"log\", xaxis=data[:, 0], xl=r\"$\\mathsf{h}$\", yl=\"Error\", title=\"Error measured with JS divergence\", savename=None)\n",
    "visualize_line(data=data[:, 2], yscale=\"log\", xaxis=data[:, 0], xl=r\"$\\mathsf{h}$\", yl=\"Error\", title=\"Error measured with KL divergence\", savename=None)\n",
    "visualize_line(data=data[:, 3], yscale=\"log\", xaxis=data[:, 0], xl=r\"$\\mathsf{h}$\", yl=\"Error\", title=\"Error measured with Wasserstein distance\", savename=None)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.10.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.10.11 64-bit"
  },
  "interpreter": {
   "hash": "a3175cd8d7d315594935f1db0dfb1a164f9f2143f6d1addea846d2ceb504a6d4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}